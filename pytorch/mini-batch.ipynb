{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df717b23-0063-40d3-9169-ea4d21cbfbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/3125], Loss: 1.4264\n",
      "Epoch [1/5], Step [101/3125], Loss: 1.0252\n",
      "Epoch [1/5], Step [201/3125], Loss: 1.0122\n",
      "Epoch [1/5], Step [301/3125], Loss: 0.9711\n",
      "Epoch [1/5], Step [401/3125], Loss: 1.0386\n",
      "Epoch [1/5], Step [501/3125], Loss: 0.6912\n",
      "Epoch [1/5], Step [601/3125], Loss: 1.2317\n",
      "Epoch [1/5], Step [701/3125], Loss: 0.9111\n",
      "Epoch [1/5], Step [801/3125], Loss: 0.8604\n",
      "Epoch [1/5], Step [901/3125], Loss: 0.7881\n",
      "Epoch [1/5], Step [1001/3125], Loss: 0.9467\n",
      "Epoch [1/5], Step [1101/3125], Loss: 1.2362\n",
      "Epoch [1/5], Step [1201/3125], Loss: 0.9764\n",
      "Epoch [1/5], Step [1301/3125], Loss: 1.2027\n",
      "Epoch [1/5], Step [1401/3125], Loss: 0.9866\n",
      "Epoch [1/5], Step [1501/3125], Loss: 1.0049\n",
      "Epoch [1/5], Step [1601/3125], Loss: 0.8862\n",
      "Epoch [1/5], Step [1701/3125], Loss: 0.8246\n",
      "Epoch [1/5], Step [1801/3125], Loss: 1.5683\n",
      "Epoch [1/5], Step [1901/3125], Loss: 0.8251\n",
      "Epoch [1/5], Step [2001/3125], Loss: 0.6822\n",
      "Epoch [1/5], Step [2101/3125], Loss: 0.8795\n",
      "Epoch [1/5], Step [2201/3125], Loss: 1.0890\n",
      "Epoch [1/5], Step [2301/3125], Loss: 0.7133\n",
      "Epoch [1/5], Step [2401/3125], Loss: 0.8887\n",
      "Epoch [1/5], Step [2501/3125], Loss: 0.7657\n",
      "Epoch [1/5], Step [2601/3125], Loss: 0.9926\n",
      "Epoch [1/5], Step [2701/3125], Loss: 0.8293\n",
      "Epoch [1/5], Step [2801/3125], Loss: 1.6067\n",
      "Epoch [1/5], Step [2901/3125], Loss: 0.7420\n",
      "Epoch [1/5], Step [3001/3125], Loss: 0.9870\n",
      "Epoch [1/5], Step [3101/3125], Loss: 1.1176\n",
      "Epoch [2/5], Step [1/3125], Loss: 1.1263\n",
      "Epoch [2/5], Step [101/3125], Loss: 1.4982\n",
      "Epoch [2/5], Step [201/3125], Loss: 0.8731\n",
      "Epoch [2/5], Step [301/3125], Loss: 1.2331\n",
      "Epoch [2/5], Step [401/3125], Loss: 0.8207\n",
      "Epoch [2/5], Step [501/3125], Loss: 1.0658\n",
      "Epoch [2/5], Step [601/3125], Loss: 1.0395\n",
      "Epoch [2/5], Step [701/3125], Loss: 0.8979\n",
      "Epoch [2/5], Step [801/3125], Loss: 0.6699\n",
      "Epoch [2/5], Step [901/3125], Loss: 1.1254\n",
      "Epoch [2/5], Step [1001/3125], Loss: 1.4598\n",
      "Epoch [2/5], Step [1101/3125], Loss: 0.4577\n",
      "Epoch [2/5], Step [1201/3125], Loss: 1.1311\n",
      "Epoch [2/5], Step [1301/3125], Loss: 0.8673\n",
      "Epoch [2/5], Step [1401/3125], Loss: 0.8328\n",
      "Epoch [2/5], Step [1501/3125], Loss: 0.6876\n",
      "Epoch [2/5], Step [1601/3125], Loss: 1.5523\n",
      "Epoch [2/5], Step [1701/3125], Loss: 1.3257\n",
      "Epoch [2/5], Step [1801/3125], Loss: 0.9293\n",
      "Epoch [2/5], Step [1901/3125], Loss: 1.0569\n",
      "Epoch [2/5], Step [2001/3125], Loss: 1.0360\n",
      "Epoch [2/5], Step [2101/3125], Loss: 0.8716\n",
      "Epoch [2/5], Step [2201/3125], Loss: 0.8203\n",
      "Epoch [2/5], Step [2301/3125], Loss: 1.2446\n",
      "Epoch [2/5], Step [2401/3125], Loss: 0.7877\n",
      "Epoch [2/5], Step [2501/3125], Loss: 1.1592\n",
      "Epoch [2/5], Step [2601/3125], Loss: 1.1091\n",
      "Epoch [2/5], Step [2701/3125], Loss: 0.7518\n",
      "Epoch [2/5], Step [2801/3125], Loss: 0.9373\n",
      "Epoch [2/5], Step [2901/3125], Loss: 1.5747\n",
      "Epoch [2/5], Step [3001/3125], Loss: 0.8344\n",
      "Epoch [2/5], Step [3101/3125], Loss: 0.9958\n",
      "Epoch [3/5], Step [1/3125], Loss: 0.8180\n",
      "Epoch [3/5], Step [101/3125], Loss: 0.8174\n",
      "Epoch [3/5], Step [201/3125], Loss: 1.0336\n",
      "Epoch [3/5], Step [301/3125], Loss: 1.1822\n",
      "Epoch [3/5], Step [401/3125], Loss: 1.1377\n",
      "Epoch [3/5], Step [501/3125], Loss: 0.9501\n",
      "Epoch [3/5], Step [601/3125], Loss: 1.0726\n",
      "Epoch [3/5], Step [701/3125], Loss: 1.2103\n",
      "Epoch [3/5], Step [801/3125], Loss: 0.7650\n",
      "Epoch [3/5], Step [901/3125], Loss: 1.0152\n",
      "Epoch [3/5], Step [1001/3125], Loss: 0.8760\n",
      "Epoch [3/5], Step [1101/3125], Loss: 0.9489\n",
      "Epoch [3/5], Step [1201/3125], Loss: 0.7938\n",
      "Epoch [3/5], Step [1301/3125], Loss: 0.5362\n",
      "Epoch [3/5], Step [1401/3125], Loss: 1.3293\n",
      "Epoch [3/5], Step [1501/3125], Loss: 0.8515\n",
      "Epoch [3/5], Step [1601/3125], Loss: 1.2986\n",
      "Epoch [3/5], Step [1701/3125], Loss: 0.8861\n",
      "Epoch [3/5], Step [1801/3125], Loss: 0.9459\n",
      "Epoch [3/5], Step [1901/3125], Loss: 1.2859\n",
      "Epoch [3/5], Step [2001/3125], Loss: 1.3673\n",
      "Epoch [3/5], Step [2101/3125], Loss: 0.8418\n",
      "Epoch [3/5], Step [2201/3125], Loss: 1.3597\n",
      "Epoch [3/5], Step [2301/3125], Loss: 1.2139\n",
      "Epoch [3/5], Step [2401/3125], Loss: 1.4290\n",
      "Epoch [3/5], Step [2501/3125], Loss: 0.6028\n",
      "Epoch [3/5], Step [2601/3125], Loss: 0.6999\n",
      "Epoch [3/5], Step [2701/3125], Loss: 0.6276\n",
      "Epoch [3/5], Step [2801/3125], Loss: 0.5430\n",
      "Epoch [3/5], Step [2901/3125], Loss: 0.7339\n",
      "Epoch [3/5], Step [3001/3125], Loss: 1.0171\n",
      "Epoch [3/5], Step [3101/3125], Loss: 0.5578\n",
      "Epoch [4/5], Step [1/3125], Loss: 0.5444\n",
      "Epoch [4/5], Step [101/3125], Loss: 0.8808\n",
      "Epoch [4/5], Step [201/3125], Loss: 0.6444\n",
      "Epoch [4/5], Step [301/3125], Loss: 1.1934\n",
      "Epoch [4/5], Step [401/3125], Loss: 1.1137\n",
      "Epoch [4/5], Step [501/3125], Loss: 1.0571\n",
      "Epoch [4/5], Step [601/3125], Loss: 0.8545\n",
      "Epoch [4/5], Step [701/3125], Loss: 1.0522\n",
      "Epoch [4/5], Step [801/3125], Loss: 0.5459\n",
      "Epoch [4/5], Step [901/3125], Loss: 1.0334\n",
      "Epoch [4/5], Step [1001/3125], Loss: 0.8665\n",
      "Epoch [4/5], Step [1101/3125], Loss: 0.6550\n",
      "Epoch [4/5], Step [1201/3125], Loss: 1.2706\n",
      "Epoch [4/5], Step [1301/3125], Loss: 1.3054\n",
      "Epoch [4/5], Step [1401/3125], Loss: 1.2243\n",
      "Epoch [4/5], Step [1501/3125], Loss: 0.9172\n",
      "Epoch [4/5], Step [1601/3125], Loss: 0.7440\n",
      "Epoch [4/5], Step [1701/3125], Loss: 1.1251\n",
      "Epoch [4/5], Step [1801/3125], Loss: 0.9772\n",
      "Epoch [4/5], Step [1901/3125], Loss: 0.9747\n",
      "Epoch [4/5], Step [2001/3125], Loss: 0.8214\n",
      "Epoch [4/5], Step [2101/3125], Loss: 1.2951\n",
      "Epoch [4/5], Step [2201/3125], Loss: 0.6067\n",
      "Epoch [4/5], Step [2301/3125], Loss: 1.0461\n",
      "Epoch [4/5], Step [2401/3125], Loss: 0.6937\n",
      "Epoch [4/5], Step [2501/3125], Loss: 0.6775\n",
      "Epoch [4/5], Step [2601/3125], Loss: 1.0788\n",
      "Epoch [4/5], Step [2701/3125], Loss: 1.0975\n",
      "Epoch [4/5], Step [2801/3125], Loss: 1.0454\n",
      "Epoch [4/5], Step [2901/3125], Loss: 0.8720\n",
      "Epoch [4/5], Step [3001/3125], Loss: 1.2541\n",
      "Epoch [4/5], Step [3101/3125], Loss: 1.0008\n",
      "Epoch [5/5], Step [1/3125], Loss: 0.5585\n",
      "Epoch [5/5], Step [101/3125], Loss: 0.8749\n",
      "Epoch [5/5], Step [201/3125], Loss: 1.3627\n",
      "Epoch [5/5], Step [301/3125], Loss: 0.8405\n",
      "Epoch [5/5], Step [401/3125], Loss: 1.1744\n",
      "Epoch [5/5], Step [501/3125], Loss: 1.0917\n",
      "Epoch [5/5], Step [601/3125], Loss: 1.0274\n",
      "Epoch [5/5], Step [701/3125], Loss: 0.6440\n",
      "Epoch [5/5], Step [801/3125], Loss: 1.0805\n",
      "Epoch [5/5], Step [901/3125], Loss: 0.9103\n",
      "Epoch [5/5], Step [1001/3125], Loss: 1.2154\n",
      "Epoch [5/5], Step [1101/3125], Loss: 1.2215\n",
      "Epoch [5/5], Step [1201/3125], Loss: 0.9046\n",
      "Epoch [5/5], Step [1301/3125], Loss: 1.6509\n",
      "Epoch [5/5], Step [1401/3125], Loss: 0.7797\n",
      "Epoch [5/5], Step [1501/3125], Loss: 1.0462\n",
      "Epoch [5/5], Step [1601/3125], Loss: 0.9621\n",
      "Epoch [5/5], Step [1701/3125], Loss: 1.0765\n",
      "Epoch [5/5], Step [1801/3125], Loss: 0.5528\n",
      "Epoch [5/5], Step [1901/3125], Loss: 0.7790\n",
      "Epoch [5/5], Step [2001/3125], Loss: 1.0950\n",
      "Epoch [5/5], Step [2101/3125], Loss: 0.9905\n",
      "Epoch [5/5], Step [2201/3125], Loss: 0.7565\n",
      "Epoch [5/5], Step [2301/3125], Loss: 1.3382\n",
      "Epoch [5/5], Step [2401/3125], Loss: 1.4175\n",
      "Epoch [5/5], Step [2501/3125], Loss: 0.7652\n",
      "Epoch [5/5], Step [2601/3125], Loss: 1.1629\n",
      "Epoch [5/5], Step [2701/3125], Loss: 0.8429\n",
      "Epoch [5/5], Step [2801/3125], Loss: 0.8655\n",
      "Epoch [5/5], Step [2901/3125], Loss: 0.7711\n",
      "Epoch [5/5], Step [3001/3125], Loss: 0.8102\n",
      "Epoch [5/5], Step [3101/3125], Loss: 1.1816\n",
      "SimpleNN(\n",
      "  (linear): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, num_samples, num_features):\n",
    "        # Generate synthetic data\n",
    "        self.X = torch.randn(num_samples, num_features)\n",
    "        self.y = torch.randn(num_samples, 1)  # Assuming a single target value per sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Simple neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)  # Simple linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def train_model(num_samples=100000, num_features=10, batch_size=32, epochs=5):\n",
    "    # Create dataset and dataloader\n",
    "    dataset = CustomDataset(num_samples, num_features)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Model, loss function, and optimizer\n",
    "    model = SimpleNN(num_features)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(dataloader):\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model()\n",
    "\n",
    "print(trained_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
