{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f6d411-1188-454e-ba2a-d9c6040ddd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    CenterCrop,\n",
    "    ToTensor,\n",
    "    Normalize,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18433950-a2c3-41df-9043-393b1a4da819",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"google/vit-base-patch16-224-in21k\"\n",
    "DATASET_NAME = \"food101\"\n",
    "MODEL_SAVE_PATH = \"./lora-food-model\"\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e15eed-ff3b-48f6-85b0-afcd55f29466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(path: str):\n",
    "    \"\"\"Calculate and print model size on disk\"\"\"\n",
    "    size = sum(os.path.getsize(f) for f in os.scandir(path) if f.is_file())\n",
    "    print(f\"Model size: {size / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506c499d-1a1c-4f52-a74d-529a380a1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model: torch.nn.Module):\n",
    "    \"\"\"Print percentage of trainable parameters\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {trainable:,}/{total:,} ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ffeff-4cfa-4ad0-9380-dc6ff2eb9447",
   "metadata": {},
   "source": [
    "The Food101 dataset contains labels (food categories) represented as integers. However, for interpretability, it’s often useful to map these integers to human-readable names.\n",
    "\n",
    "label2id: A dictionary that maps each label name (e.g., \"pizza\", \"burger\") to its corresponding integer ID.\n",
    "\n",
    "id2label: A dictionary that maps each integer ID back to its label name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb64c9ea-25e7-423b-8434-6fa05f07ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "def load_and_prepare_dataset():\n",
    "    \"\"\"\n",
    "    1. Load Food101 dataset (10,000 samples)\n",
    "    2. Create train/test split (90/10)\n",
    "    3. Create label mappings\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train[:10000]\")\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "    \n",
    "    # Create label mappings\n",
    "    label2id = {label: i for i, label in enumerate(dataset[\"train\"].features[\"label\"].names)}\n",
    "    id2label = {i: label for i, label in enumerate(dataset[\"train\"].features[\"label\"].names)}\n",
    "    \n",
    "    return dataset[\"train\"], dataset[\"test\"], label2id, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2c0ba-1983-4d3f-b860-09384c7e997a",
   "metadata": {},
   "source": [
    "Initialize Image Processor:\n",
    "* Loads a preconfigured AutoImageProcessor for the specified Vision Transformer (ViT) model (e.g., \"google/vit-base-patch16-224\").\n",
    "* The processor contains model-specific parameters like:\n",
    "* Expected input size (processor.size[\"height\"] and processor.size[\"width\"]).\n",
    "* Normalization statistics (image_mean, image_std) used during the model’s original training.\n",
    "\n",
    "Define Transformation Pipeline:\n",
    "* Resize(processor.size[\"height\"]): Resizes images to match the model’s expected input dimensions (e.g., 224x224 for ViT-Base).\n",
    "* CenterCrop(processor.size[\"height\"]): Crops the center of the resized image to ensure a fixed size (e.g., 224x224) even if the original aspect ratio is different.\n",
    "* ToTensor(): Converts the image from PIL format to a PyTorch tensor (with shape [C, H, W] and values in [0, 1]).\n",
    "* Normalize(...): Normalizes the tensor using the model’s pretraining statistic\n",
    "\n",
    "Batch Processing Logic:\n",
    "* batch[\"image\"]: Assumes the input batch contains a list of PIL images under the key \"image\".\n",
    "* img.convert(\"RGB\"): Ensures all images are in RGB format (3 channels), even if some are grayscale or have alpha channels.\n",
    "* preprocess_pipeline(img): Applies the transformation pipeline (resize, crop, tensor conversion, normalization) to each image.\n",
    "* batch[\"pixel_values\"]: Stores the processed tensors under the key \"pixel_values\", which is the expected input format for ViT models in Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ae6e9c-59c8-472c-a247-d2669c9758a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "def create_preprocessing_pipeline():\n",
    "    \"\"\"\n",
    "    Create transformation pipeline matching ViT requirements:\n",
    "    1. Resize to 224x224\n",
    "    2. Center crop\n",
    "    3. Convert to tensor\n",
    "    4. Normalize with ImageNet stats\n",
    "    \"\"\"\n",
    "    processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "    return Compose([\n",
    "        Resize(processor.size[\"height\"]),\n",
    "        CenterCrop(processor.size[\"height\"]),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "\n",
    "def build_preprocess_batch(preprocess_pipeline):\n",
    "    \"\"\"Build a function that applies preprocessing to batch of images\"\"\"\n",
    "    def preprocess_batch(batch):\n",
    "        batch[\"pixel_values\"] = [preprocess_pipeline(img.convert(\"RGB\")) \n",
    "                                for img in batch[\"image\"]]\n",
    "        return batch\n",
    "    return preprocess_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed35b9-3135-46da-9d03-7931e4a5817e",
   "metadata": {},
   "source": [
    "The function sets up the pre-trained Vision Transformer (ViT) model with Low-Rank Adaptation (LoRA) for fine-tuning.\n",
    "It takes label mapping dictionaries (label2id and id2label) as input\n",
    "\n",
    "AutoModelForImageClassification.from_pretrained\n",
    "* loads a pre-trained ViT model\n",
    "* label2id and id2label: Mappings between class names and integer IDs (required for the classification head).\n",
    "* ignore_mismatched_sizes=True allows the model to resize its classification head if the number of classes in the pre-trained model differs from the target dataset\n",
    "  \n",
    "LoraConfig defines how LoRA adapters are applied to the model.\n",
    "LoRA adaptation parameters:\n",
    "* r=16: Sets the rank of the low-rank matrices to 16 (higher values = more capacity but more parameters)\n",
    "* lora_alpha=16: Sets the scaling factor for the LoRA updates (typically set equal to r)\n",
    "* target_modules=[\"query\", \"value\"]: Specifies which attention matrices to adapt (only modifying the query and value matrices in the self-attention mechanism, not the key matrices)\n",
    "* lora_dropout=0.1: Adds 10% dropout to LoRA layers for regularization during training\n",
    "* bias=\"none\": Doesn't apply LoRA to bias parameters\n",
    "* modules_to_save=[\"classifier\"]: Ensures the classifier layer (the final classification head) remains fully trainable, not frozen or replaced by LoRA.\n",
    "\n",
    "get_peft_model injects LoRA into the base_model based on lora_config.\n",
    "\n",
    "print_trainable_parameters: Outputs the percentage of trainable parameters (e.g., 0.1% instead of 100% for full fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b739b12-8000-4d7c-a567-7d8e4560396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Setup\n",
    "def initialize_model(label2id, id2label):\n",
    "    \"\"\"\n",
    "    Initialize model with LoRA adaptation:\n",
    "    1. Load pre-trained ViT\n",
    "    2. Add LoRA to query and value layers\n",
    "    3. Keep classifier layer trainable\n",
    "    \"\"\"\n",
    "    # Load base model\n",
    "    base_model = AutoModelForImageClassification.from_pretrained(\n",
    "        MODEL_CHECKPOINT,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        # LoRA rank\n",
    "        r=16,                  \n",
    "        # Scaling factor\n",
    "        lora_alpha=16,        \n",
    "        # Only modify specific attention matrices\n",
    "        target_modules=[\"query\", \"value\"],  \n",
    "        # Regularization\n",
    "        lora_dropout=0.1,     \n",
    "        # No bias params\n",
    "        bias=\"none\",          \n",
    "        # Full fine-tune the classifier head\n",
    "        modules_to_save=[\"classifier\"],  \n",
    "    )\n",
    "    \n",
    "    # Create LoRA model\n",
    "    lora_model = get_peft_model(base_model, lora_config)\n",
    "    print_trainable_parameters(lora_model)\n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d808e4-8009-49c2-bb18-f6c019e408e7",
   "metadata": {},
   "source": [
    "configure_training_parameters sets up all the configuration needed for efficiently training a Vision Transformer model with LoRA.\n",
    "\n",
    "TrainingArguments\n",
    "* output_dir=\"./checkpoints\": Specifies where to save model checkpoints during training\n",
    "* per_device_train_batch_size=BATCH_SIZE: Sets the batch size for training, using a predefined constant\n",
    "* per_device_eval_batch_size=BATCH_SIZE: Sets the batch size for evaluation, using the same value\n",
    "* gradient_accumulation_steps=4: Accumulates gradients over 4 batches before updating weights, effectively increasing the batch size by 4x without increasing memory usage\n",
    "* fp16=True: Enables mixed precision training (using 16-bit floating-point numbers where possible), which speeds up training and reduces memory usage\n",
    "* learning_rate=5e-3: Sets a relatively high learning rate (0.005), which is appropriate for LoRA fine-tuning since fewer parameters are being updated\n",
    "* num_train_epochs=EPOCHS: Sets the number of full passes through the training data\n",
    "* logging_steps=10: Logs training metrics every 10 training steps\n",
    "* evaluation_strategy=\"epoch\": Evaluates the model on the test dataset after each epoch\n",
    "* save_strategy=\"epoch\": Saves a checkpoint after each epoch\n",
    "* load_best_model_at_end=True: Loads the best-performing model (by evaluation metrics) at the end of training\n",
    "\n",
    "compute_metrics:\n",
    "* Takes model predictions and reference labels as input\n",
    "* Converts the raw prediction logits to class predictions by taking the argmax along the class dimension\n",
    "* Computes and returns the accuracy by comparing predictions to reference labels\n",
    "\n",
    "collate_fn:\n",
    "* Takes a batch of examples (each containing processed image tensors and labels)\n",
    "* Stacks the \"pixel_values\" from each example into a single tensor batch\n",
    "* Converts the labels into a tensor\n",
    "* Returns a dictionary with the batched tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1badee1c-47c5-45db-8ba9-b74b585dd752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "def configure_training_parameters(train_dataset, test_dataset):\n",
    "    \"\"\"\n",
    "    Configure training parameters:\n",
    "    - Mixed precision training\n",
    "    - Gradient accumulation\n",
    "    - Batch processing\n",
    "    - Model evaluation\n",
    "    \"\"\"        \n",
    "    args = TrainingArguments(\n",
    "        # Output directory\n",
    "        output_dir=\"./checkpoints\",\n",
    "        # Batch size\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        # Gradient accumulation\n",
    "        gradient_accumulation_steps=4,\n",
    "        # Mixed precision training\n",
    "        fp16=use_fp16,\n",
    "        # Learning rate\n",
    "        learning_rate=5e-3,\n",
    "        # Number of training epochs\n",
    "        num_train_epochs=EPOCHS,\n",
    "        # Logging steps\n",
    "        logging_steps=10,\n",
    "        # Evaluation strategy\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        # Save strategy\n",
    "        save_strategy=\"epoch\",\n",
    "        # Load best model at end\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    # Accuracy metric\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        # Get predictions\n",
    "        predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "        # Compute accuracy\n",
    "        return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    \n",
    "    # Data collator for batching\n",
    "    def collate_fn(examples):\n",
    "        return {\n",
    "            \"pixel_values\": torch.stack([e[\"pixel_values\"] for e in examples]),\n",
    "            \"labels\": torch.tensor([e[\"label\"] for e in examples]),\n",
    "        }\n",
    "    \n",
    "    return args, compute_metrics, collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a676a0-4ded-4766-98e4-f43df7e4198d",
   "metadata": {},
   "source": [
    "predict function handles the entire inference process for classifying food images with the LoRA-adapted Vision Transformer model\n",
    "\n",
    "Steps:\n",
    "* Loads the image processor from the original model checkpoint to ensure preprocessing is consistent with what the model expects.\n",
    "* Loads the base Vision Transformer model, with ignore_mismatched_sizes=True to handle any potential mismatches between the original classification head and the fine-tuned one.\n",
    "* Loads and applies the trained LoRA adapter weights from MODEL_SAVE_PATH to the base model using the PeftModel.from_pretrained() method.\n",
    "* Opens the image file from the provided path using PIL's Image.open. Converts the image to RGB format. Returns the processed inputs in PyTorch tensor format with batch dimension\n",
    "* Passes the processed inputs to the model and collects the outputs. outputs.logits: Contains unnormalized prediction scores (logits) for all food classes.\n",
    "* Finds the index of the highest logit value using argmax(), which represents the predicted class ID.\n",
    "* Converts this numeric ID back to a human-readable food class label using the model's id2label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f20a00-ceb8-451b-a20a-c743f691988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference/Prediction Function\n",
    "def predict(image_path: str):\n",
    "    \"\"\"\n",
    "    Predict food class from image:\n",
    "    1. Load trained LoRA adapter\n",
    "    2. Preprocess image\n",
    "    3. Run inference\n",
    "    \"\"\"\n",
    "    # Load processor and base model\n",
    "    processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "    base_model = AutoModelForImageClassification.from_pretrained(\n",
    "        MODEL_CHECKPOINT,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter\n",
    "    model = PeftModel.from_pretrained(base_model, MODEL_SAVE_PATH)\n",
    "    \n",
    "    # Preprocess image\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(image.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get predicted class\n",
    "    predicted_id = outputs.logits.argmax().item()\n",
    "    return model.config.id2label[predicted_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70ba482a-5cb1-40df-ae71-59303e814554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "train_dataset, test_dataset, label2id, id2label = load_and_prepare_dataset()\n",
    "\n",
    "print(type(train_dataset))\n",
    "print(type(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b577fb02-2d50-48a0-b08c-aeccbf5b27b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocess_pipeline = create_preprocessing_pipeline()\n",
    "\n",
    "# Apply preprocessing\n",
    "batch_preprocess_fn = build_preprocess_batch(preprocess_pipeline)\n",
    "train_dataset.set_transform(batch_preprocess_fn)\n",
    "test_dataset.set_transform(batch_preprocess_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78db7fa-2019-45a9-9dc2-8ec309da4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = initialize_model(label2id, id2label)\n",
    "\n",
    "# Set up training\n",
    "args, compute_metrics, collate_fn = configure_training_parameters(train_dataset, test_dataset)\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa386e57-5b2a-4cc4-ae6c-5083eec9b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449028e-aee3-4558-8cdf-21f785b80fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and save\n",
    "results = trainer.evaluate()\n",
    "print(f\"Final accuracy: {results['eval_accuracy']:.2%}\")\n",
    "trainer.save_model(MODEL_SAVE_PATH)\n",
    "print_model_size(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b614c0-c6ef-4563-b88c-e159f69a37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "test_image = \"path_to_your_image.jpg\"\n",
    "print(f\"Prediction: {predict(test_image)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
